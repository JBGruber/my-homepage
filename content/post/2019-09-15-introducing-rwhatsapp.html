---
title: Introducing rwhatsapp
author: Johannes Gruber
date: '2019-09-23'
slug: introducing-rwhatsapp
categories: []
tags:
  - R
  - text-analysis
header:
  caption: ''
  image: 'emojis-1.png'
  preview: yes
---



<p>Iâ€™m happy to announce that <code>rwhatsapp</code> is now on <code>CRAN</code>.
After being tested by users on GitHub for a year now, I decided it is time to make the package available to a wider audience.
The goal of the package is to make working with â€˜WhatsAppâ€™ chat logs as easy as possible.</p>
<p>â€˜WhatsAppâ€™ seems to become increasingly important not just as a messaging service but also as a social networkâ€”thanks to its group chat capabilities.
Furthermore, retrieving chat logs from the Android or iOS app is very straightforward:
Simply choose <code>More</code> in the menu of a chat, then <code>Export chat</code> and export the history to a txt file.</p>
<p><img src="https://i.imgur.com/9pZjPFC.jpg" width="200" /> <img src="https://i.imgur.com/OwUE6aE.jpg" width="200" /> <img src="https://i.imgur.com/8lCJQfZ.jpg" width="200" /></p>
<p>While this is easy enough to do, please make sure to ask other chat participants for consent before working with their data.</p>
<p>Install the package with:</p>
<pre class="r"><code>install.packages(&quot;rwhatsapp&quot;)</code></pre>
<p>The package comes with a small sample that you can use to get going.</p>
<pre class="r"><code>library(&quot;rwhatsapp&quot;)
history &lt;- system.file(&quot;extdata&quot;, &quot;sample.txt&quot;, package = &quot;rwhatsapp&quot;)
history</code></pre>
<pre><code>## [1] &quot;/home/johannes/R/x86_64-pc-linux-gnu-library/3.6/rwhatsapp/extdata/sample.txt&quot;</code></pre>
<p>The main function of the package, <code>rwa_read()</code> can handle <code>txt</code> (and <code>zip</code>) files directly, which means that you can simply provide the path to a file to get started:</p>
<pre class="r"><code>chat &lt;- rwa_read(history)
chat</code></pre>
<pre><code>## # A tibble: 9 x 6
##   time                author   text          source        emoji emoji_name
##   &lt;dttm&gt;              &lt;fct&gt;    &lt;chr&gt;         &lt;chr&gt;         &lt;lis&gt; &lt;list&gt;    
## 1 2017-07-12 22:35:19 &lt;NA&gt;     Messages to â€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 2 2017-07-12 22:35:19 &lt;NA&gt;     &quot;You createdâ€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 3 2017-07-12 22:35:19 Johanneâ€¦ &lt;Media omittâ€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 4 2017-07-12 22:35:19 Johanneâ€¦ Fruit bread â€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [2]&gt; 
## 5 2017-07-13 09:12:19 Test     &quot;It&#39;s fun doâ€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 6 2017-07-13 09:16:19 Johanneâ€¦ Haha it sureâ€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [1]&gt; 
## 7 2018-09-28 13:27:19 Johanneâ€¦ Did you knowâ€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 8 2018-09-28 13:28:19 Johanneâ€¦ ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£â˜ºğŸ˜ŠğŸ˜‡ğŸ™‚â€¦ /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [242â€¦
## 9 2018-09-28 13:30:19 Johanneâ€¦ ğŸ¤·â€â™€ğŸ¤·ğŸ»â€â™‚ğŸ™â€â™€ğŸ™â€â€¦     /home/johannâ€¦ &lt;chrâ€¦ &lt;chr [87]&gt;</code></pre>
<p>This is really all the package does: bring your chat logs into a nice usable format.
But since this isnâ€™t very intresting, I decided to also show you a few quick analysis steps to illustrate what you can do with data obtained in this way.
For this, I use one of my own chat logs from a conversation with friends.
I wonâ€™t share it but you should be able to follow along with your own data.</p>
<pre class="r"><code>chat &lt;- rwa_read(&quot;/home/johannes/WhatsApp Chat.txt&quot;)
chat</code></pre>
<pre><code>## # A tibble: 16,816 x 6
##    time                author   text             source    emoji emoji_name
##    &lt;dttm&gt;              &lt;fct&gt;    &lt;chr&gt;            &lt;chr&gt;     &lt;lis&gt; &lt;list&gt;    
##  1 2015-12-10 19:57:19 Artur Kâ€¦ &lt;Media omitted&gt;  /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
##  2 2015-12-10 22:31:19 Erika Iâ€¦ ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚     /home/joâ€¦ &lt;chrâ€¦ &lt;chr [6]&gt; 
##  3 2015-12-11 02:13:19 Alexandâ€¦ ğŸ™ˆ               /home/joâ€¦ &lt;chrâ€¦ &lt;chr [1]&gt; 
##  4 2015-12-11 02:23:19 Johanneâ€¦ ğŸ˜‚               /home/joâ€¦ &lt;chrâ€¦ &lt;chr [1]&gt; 
##  5 2015-12-11 02:24:19 Johanneâ€¦ Die Petitionen â€¦ /home/joâ€¦ &lt;chrâ€¦ &lt;chr [1]&gt; 
##  6 2015-12-11 03:51:19 Erika Iâ€¦ LÃ¤Ã¤Ã¤uft          /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
##  7 2015-12-12 07:49:19 Johanneâ€¦ &lt;Media omitted&gt;  /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
##  8 2015-12-12 07:53:19 Erika Iâ€¦ was macht ihr hâ€¦ /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
##  9 2015-12-12 07:55:19 Johanneâ€¦ Alex arbeitet wâ€¦ /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## 10 2015-12-12 07:55:19 Johanneâ€¦ und ich spiele â€¦ /home/joâ€¦ &lt;chrâ€¦ &lt;chr [0]&gt; 
## # â€¦ with 16,806 more rows</code></pre>
<p>You can see from the size of the resulting <code>data.frame</code> that we write a lot in this group!
Letâ€™s see over how much time we managed to accumulate 16,816 messages.
I use a couple of extra packages for that:</p>
<pre class="r"><code>library(&quot;dplyr&quot;)
library(&quot;ggplot2&quot;); theme_set(theme_bw())
library(&quot;lubridate&quot;)
chat %&gt;%
  mutate(day = date(time)) %&gt;%
  count(day) %&gt;%
  ggplot(aes(x = day, y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;&quot;) + xlab(&quot;&quot;) +
  ggtitle(&quot;Messages per day&quot;)</code></pre>
<p><img src="https://i.imgur.com/Q3hVhWl.png" width="672" /></p>
<p>The chat has been going on for a while and on some days there were more than a hundred messages!
Whoâ€™s responsible for all of this?</p>
<pre class="r"><code>chat %&gt;%
  mutate(day = date(time)) %&gt;%
  count(author) %&gt;%
  ggplot(aes(x = reorder(author, n), y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;&quot;) + xlab(&quot;&quot;) +
  coord_flip() +
  ggtitle(&quot;Number of messages&quot;)</code></pre>
<p><img src="https://i.imgur.com/WSrchwt.png" width="672" /></p>
<p>Looks like we contributed more or less the same number of messages, with Erika slightly leading the field.</p>
<p>One thing that is always fun to do is finding out what peopleâ€™s favourite emojis are:</p>
<pre class="r"><code>library(&quot;tidyr&quot;)
chat %&gt;%
  unnest(emoji) %&gt;%
  count(author, emoji, sort = TRUE) %&gt;%
  group_by(author) %&gt;%
  top_n(n = 6, n) %&gt;%
  ggplot(aes(x = reorder(emoji, n), y = n, fill = author)) +
  geom_col(show.legend = FALSE) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;) +
  coord_flip() +
  facet_wrap(~author, ncol = 2, scales = &quot;free_y&quot;)  +
  ggtitle(&quot;Most often used emojis&quot;)</code></pre>
<p><img src="https://i.imgur.com/CENEu0T.png" width="672" /></p>
<p>Looks like we have a clear winner: all of us like the :joy: (â€œface with tears of joyâ€) most.
:sweat_smile: (â€œgrinning face with sweatâ€) is also very popular, except with Erika who has a few more flamboyant favourites.
I apparently tend to use fewer emojis overall while Erika is leading the field (again).</p>
<p>How does it look if we compare favourite words?
I use the excellent <code>tidytext</code> package to get this task done:</p>
<pre class="r"><code>library(&quot;tidytext&quot;)
chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  count(author, word, sort = TRUE) %&gt;%
  group_by(author) %&gt;%
  top_n(n = 6, n) %&gt;%
  ggplot(aes(x = reorder(word, n), y = n, fill = author)) +
  geom_col(show.legend = FALSE) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;) +
  coord_flip() +
  facet_wrap(~author, ncol = 2, scales = &quot;free_y&quot;) +
  ggtitle(&quot;Most often used words&quot;)</code></pre>
<p><img src="https://i.imgur.com/Cfncor5.png" width="672" /></p>
<p>This doesnâ€™t make much sense.
First of all, because we write in German which you might not understand :wink:.
But it also looks weird that Artur and Erika seem to often use the words â€œmediaâ€ and â€œomittedâ€.
Of course, this is just the placeholder WhatsApp puts into the log file instead of a picture or video.
But the other words donâ€™t look particularly useful either.
They are whatâ€™s commonly called stopwords: words that are used often but donâ€™t carry any substantial meaning.
â€œundâ€ for example is simply â€œandâ€ in English.
â€œderâ€, â€œdieâ€ and â€œdasâ€ all mean â€œtheâ€ in English (which makes German pure joy to learn for an English native speaker :sweat_smile:).</p>
<p>To get around this mess, I remove these words before making the plot again:</p>
<pre class="r"><code>library(&quot;stopwords&quot;)
to_remove &lt;- c(stopwords(language = &quot;de&quot;),
               &quot;media&quot;,
               &quot;omitted&quot;,
               &quot;ref&quot;,
               &quot;dass&quot;,
               &quot;schon&quot;,
               &quot;mal&quot;,
               &quot;android.s.wt&quot;)

chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  filter(!word %in% to_remove) %&gt;%
  count(author, word, sort = TRUE) %&gt;%
  group_by(author) %&gt;%
  top_n(n = 6, n) %&gt;%
  ggplot(aes(x = reorder(word, n), y = n, fill = author)) +
  geom_col(show.legend = FALSE) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;) +
  coord_flip() +
  facet_wrap(~author, ncol = 2, scales = &quot;free_y&quot;) +
  ggtitle(&quot;Most often used words&quot;)</code></pre>
<p><img src="https://i.imgur.com/xnokaU7.png" width="672" /></p>
<p>Still not very informative, but hey, this is just a private conversation, what did you expect?
It seems though that we agree with each other a lot, as â€œjaâ€ (yes) and ok are among the top words for all of us.
The antonym â€œneâ€ (nope) is far less common and only on Arturâ€™s and Erikaâ€™s top lists.
I seem to send a lot of links as both â€œhttpsâ€ and â€œrefâ€ appear on my top list.
Alexandra is talking to or about Erika and me pretty often and Artur is the only one who mentions â€œeuroâ€ (as in the currency) pretty often.</p>
<p>Another way to determine favourite words is to calculate the term frequencyâ€“inverse document frequency (tfâ€“idf).
Basically, what the measure does, in this case, is to find words that are common within the messages of one author but uncommon in the rest of the messages.</p>
<pre class="r"><code>chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  select(word, author) %&gt;%
  filter(!word %in% to_remove) %&gt;%
  mutate(word = gsub(&quot;.com&quot;, &quot;&quot;, word)) %&gt;%
  mutate(word = gsub(&quot;^gag&quot;, &quot;9gag&quot;, word)) %&gt;%
  count(author, word, sort = TRUE) %&gt;%
  bind_tf_idf(term = word, document = author, n = n) %&gt;%
  filter(n &gt; 10) %&gt;%
  group_by(author) %&gt;%
  top_n(n = 6, tf_idf) %&gt;%
  ggplot(aes(x = reorder(word, n), y = n, fill = author)) +
  geom_col(show.legend = FALSE) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;) +
  coord_flip() +
  facet_wrap(~author, ncol = 2, scales = &quot;free_y&quot;) +
  ggtitle(&quot;Important words using tfâ€“idf by author&quot;)</code></pre>
<p><img src="https://i.imgur.com/I7as1WQ.png" width="672" /></p>
<p>Now the picture changes pretty much entirely.
First, the top words of the different authors have very little overlap now compared to beforeâ€”only exceptions being 9gag (platform to share memes) in Alexandraâ€™s and my messages and â€œgradeâ€ (now) which Artur and I use.
This is due to the tfâ€“idf measure which tries to find only words specific to an author.</p>
<p>Now instead of Erika and me, Alexandra talks about Artur, something only she does.
Artur is the only one to talk about a Macbook (as he is the only one who owns one).
Erika seems to thrive on abbreviations like â€œomanâ€ (abbreviation for â€œOh Mannâ€/â€œoh manâ€, not the country) â€œeigâ€ (â€œeigentlichâ€/actually) â€œjohâ€ (abbreviation for my name) and curiously â€œjaaâ€, which is â€œjaâ€ (yes) with and unnecessary extra â€œaâ€.
I show that my favourite adjective is â€œsuperâ€ and that I talked about a processor at some point for some reason.</p>
<p>Another common text mining tool is to calculate lexical diversity.
Basically, you just check how many unique words are used by an author.</p>
<pre class="r"><code>chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  filter(!word %in% to_remove) %&gt;%
  group_by(author) %&gt;%
  summarise(lex_diversity = n_distinct(word)) %&gt;%
  arrange(desc(lex_diversity)) %&gt;%
  ggplot(aes(x = reorder(author, lex_diversity),
                          y = lex_diversity,
                          fill = author)) +
  geom_col(show.legend = FALSE) +
  scale_y_continuous(expand = (mult = c(0, 0, 0, 500))) +
  geom_text(aes(label = scales::comma(lex_diversity)), hjust = -0.1) +
  ylab(&quot;unique words&quot;) +
  xlab(&quot;&quot;) +
  ggtitle(&quot;Lexical Diversity&quot;) +
  coord_flip()</code></pre>
<p><img src="https://i.imgur.com/mI9Vcky.png" width="672" /></p>
<p>It appears that I use the most unique words, even though Erika wrote more messages overall.
Is this because I use some amazing and unique technical terms?
Letâ€™s find out:</p>
<pre class="r"><code>o_words &lt;- chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  filter(author != &quot;Johannes&quot;) %&gt;% 
  count(word, sort = TRUE) 

chat %&gt;%
  unnest_tokens(input = text,
                output = word) %&gt;%
  filter(author == &quot;Johannes&quot;) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  filter(!word %in% o_words$word) %&gt;% # only select words nobody else uses
  top_n(n = 6, n) %&gt;%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(show.legend = FALSE) +
  ylab(&quot;&quot;) + xlab(&quot;&quot;) +
  coord_flip() +
  ggtitle(&quot;Unique words of Johannes&quot;)</code></pre>
<p><img src="https://i.imgur.com/H2R7mZq.png" width="672" /></p>
<p>Looking at the top words that are only used by me we see these are words I donâ€™t use very often either.
There are two technical terms here: â€œprozessorâ€ and â€œwebseiteâ€ which kind of make sense.
Iâ€™m also apparently the only one to share links to the German news site zeit.de.
The English â€œiâ€™mâ€ is in there because autocorrect on my phone tends to change the German word â€œimâ€ (in).</p>
<p>Overall, WhatsApp data is just a fun source to play around with text mining methods.
But if you have more serious data, a proper text analysis is also possible, just like with other social media data.</p>

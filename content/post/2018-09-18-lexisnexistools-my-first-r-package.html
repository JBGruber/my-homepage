---
title: LexisNexisTools. My first `R` package
author: Johannes B. Gruber
date: '2018-05-19'
slug: lexisnexistools
categories: []
tags:
  - R
header:
  caption: ''
  image: ''
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---



<p>My PhD supervisor once told me that everyone doing newspaper analysis starts by writing code to read in files from the ‘LexisNexis’ newspaper archive. However, while I do recommend this exercise, not everyone has the time.</p>
<p>These are the first words of the introduction to my first <code>R</code> package, LexisNexisTools. My PhD supervisor was also my supervisor for my master dissertation and he said these words before he gave me my very first book about <code>R</code>. After getting the initial bits of code from him, I worked with files from LexisNexis first for my dissertation, then in a project where we used LexisNexis data and since I’m also using newspaper data for my PhD project, I knew I would be using the code again.</p>
<p>At some point, I decided that I would spare others the hassle of figuring out all the quirks of files that come from LexisNexis. I made the decision to write an <code>R</code> package instead of just sharing the code because of two reasons: 1. I wanted that others who stumble across my work have a chance to report any issues and bugs they find. This was at first a scary thought: my work would be scrutinised by others on the internet! But after some deliberation I concluded that someone calling out my sloppy code is much better than if I find an error myself months down the line when it means that I have to re-do all the work to make it right. 2. I admire people who write good packages since they make my own life much easier. I wanted to give something back to the community.</p>
<p>One might think that the process of reading files into <code>R</code> which are in the txt format is not harder than using the command <code>readLines()</code>. Yet, while this gets you a nice character object, LexisNexis puts up to 500 articles into one file, together with the meta data of each article. To get this into a spreadsheet like format and to get all the data types correctly can be a great hassle and definitely caused me some headache. Especially since LexisNexis seems to have changed the file structure a couple of times over the years. They use different structures for different sources and on top of that are inconsistent with the order of meta data.</p>
<p>The task was thus to find the structure in the files. For those who haven’t seen a file from LexisNexis, here is a small example:</p>
<p align="center">
<a href="https://ibb.co/fj5YjG"><img src="https://preview.ibb.co/fOfNdb/LN.png" alt="LN" border="1"></a>
</p>
<p>After the cover sheet, the first article starts with <em>1 of 87 DOCUMENTS</em>. The 87 can of course change but the format is always the same, which means the beginning of an article is always (at least for the English version) <code>\\d+ of \\d+ DOCUMENTS$</code>. This information can be used to chop up the file into individual articles.</p>
<pre class="r"><code>beginnings &lt;- which(stringi::stri_detect_regex(articles.v, &quot;\\d+ of \\d+ DOCUMENTS$&quot;))
articles.l &lt;- lapply(seq_along(beginnings), function(n) {
  if (n &lt; length(beginnings)) {
    articles.v[beginnings[n]:(beginnings[n + 1] - 1)]
  } else {
    articles.v[beginnings[n]:length(articles.v)]
  }
})</code></pre>
<p><code>articles.v</code> here is simply the the read-in file as a character vector. The <code>lapply()</code> loop goes through the instances in beginnings and constructs a list where each entry starts at the line with <code>\\d+ of \\d+ DOCUMENTS$</code> and ends one line before the next article begins. At the last iteration of the loop, where <code>n</code> is equal to <code>length(beginnings)</code>, the list entry will contain the text from the last <code>\\d+ of \\d+ DOCUMENTS$</code> until the end of the file.</p>
<p>Each entry is then further divided at a second crucial keyword: <code>LENGTH</code>. This marks the end of the metadata and hence can be use to separate article text from meta information provided by LexisNexis.</p>
<p>Then a number of commands perform some Regex magic or use experience about the position of certain information to put the pieces in columns.</p>
<p>But users don’t have to worry about any of this any more. They can simply use:</p>
<pre class="r"><code>LexisNexisTools::lnt_read()</code></pre>
<p>You can check out a quick walk-through at <a href="https://github.com/JBGruber/LexisNexisTools">github.com/JBGruber/LexisNexisTools</a>.</p>
<p>I’m really proud of my first package. Much of the code already existed before I first posted it online but seeing and hearing that others find it useful gave me a major boost of confidence. So far nobody has complained about the code base and I’m happy all the work I put into it does not just sit on my own computer but others can profit.</p>
